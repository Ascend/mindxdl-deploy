apiVersion: mindxdl.gitee.com/v1
kind: AscendJob
metadata:
  name: default-test-pytorch
  labels:
    framework: pytorch
    ring-controller.atlas: ascend-910b
    fault-scheduling: "grace"
    tor-affinity: "null" #该标签为任务是否使用交换机亲和性调度标签，null或者不写该标签则不适用。large-model-schema表示大模型任务，normal-schema 普通任务
spec:
  schedulerName: volcano  # work when enableGangScheduling is true
  runPolicy:
    schedulingPolicy:    # work when enableGangScheduling is true
      minAvailable: 1
      queue: default
  successPolicy: AllWorkers
  replicaSpecs:
    Master:
      replicas: 1
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            ring-controller.atlas: ascend-910b
        spec:
          terminationGracePeriodSeconds: 900
          nodeSelector:
            host-arch: huawei-arm
            accelerator-type: card-910b-2   # depend on your device model, 910bx8 is module-910b-8 ,910bx16 is module-910b-16
          hostNetwork: true
          containers:
          - name: ascend # do not modify
            image: pytorch-test:latest      # training framework image， which can be modified
            imagePullPolicy: IfNotPresent
            env:
              - name: XDL_IP                                       # IP address of the physical node, which is used to identify the node where the pod is running
                valueFrom:
                  fieldRef:
                    fieldPath: status.hostIP
              # ASCEND_VISIBLE_DEVICES env variable is used by ascend-docker-runtime when in the whole card scheduling scene with volcano scheduler. please delete it when in the static vNPU scheduling scene or without volcano.
              - name: ASCEND_VISIBLE_DEVICES
                valueFrom:
                  fieldRef:
                    fieldPath: metadata.annotations['huawei.com/Ascend910']               # The value must be the same as resources.requests
            command:                        # training command, which can be modified
              - /bin/bash
              - -c
            args: ["cd /job/code; chmod +x scripts/train_start.sh; bash scripts/train_start.sh /job/code /job/output pretrain_gpt.py --num-layers=8 --hidden-size=12288 --num-attention-heads=96 --micro-batch-size=4 --global-batch-size=96 --seq-length=1024 --max-position-embeddings=1024 --train-iters=500000 --lr-decay-iters=320000 --save=./checkpoint_dist --load=./checkpoint_dist --data-path=/job/data/enwiki/my-t5_text_sentence --tensor-model-parallel-size=8 --use-distributed-optimizer --pipeline-model-parallel-size=8 --vocab-file=/job/code/gpt2-vocab.json --merge-file=/job/code/gpt2-merges.txt --data-impl=mmap --split=949,50,1 --distributed-backend=nccl --lr=0.375e-5 --lr-decay-style=cosine --min-lr=0.375e-6 --weight-decay=0.1 --clip-grad=1.0 --lr-warmup-fraction=.01 --adam-beta1=0.9 --adam-beta2=0.95 --init-method-std=0.006 --recompute-granularity=full --recompute-method=uniform --no-gradient-accumulation-fusion --log-interval=1 --save-interval=10000 --eval-interval=1000 --eval-iters=10 --fp16;"]
            ports:                          # default value containerPort: 2222 name: ascendjob-port if not set
              - containerPort: 2222         # determined by user
                name: ascendjob-port        # do not modify
            resources:
              limits:
                huawei.com/Ascend910: 8
              requests:
                huawei.com/Ascend910: 8
            volumeMounts:
            - name: code
              mountPath: /job/code
            - name: data
              mountPath: /job/data
            - name: output
              mountPath: /job/output
            - name: ascend-driver
              mountPath: /usr/local/Ascend/driver
            - name: ascend-add-ons
              mountPath: /usr/local/Ascend/add-ons
            - name: dshm
              mountPath: /dev/shm
            - name: localtime
              mountPath: /etc/localtime
          volumes:
          - name: code
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/code/GPT-3_for_PyTorch_1.11_code/"
          - name: data
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/public/dataset/"
          - name: output
            nfs:
              server: 127.0.0.1
              path: "/data/atlas_dls/output/"
          - name: ascend-driver
            hostPath:
              path: /usr/local/Ascend/driver
          - name: ascend-add-ons
            hostPath:
              path: /usr/local/Ascend/add-ons
          - name: dshm
            emptyDir:
              medium: Memory
          - name: localtime
            hostPath:
              path: /etc/localtime


